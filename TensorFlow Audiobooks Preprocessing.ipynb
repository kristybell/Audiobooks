{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCj9a5IF4Aj"
      },
      "source": [
        "# The Business Action Plan:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSvuhnKSF4Ao"
      },
      "source": [
        "1. Preprocess the data\n",
        "    - In a data science team there may be a person whose sole job is to preprocess datasets\n",
        "    - Common Techniques:\n",
        "         1. Balance the dataset\n",
        "         2. Divide the dataset in training, validation, and test (to prevent overfitting\n",
        "         3. Save the data in a tensor friendly format (good old .npz)\n",
        "2. Create the machine learning algorithm\n",
        "   - use the same structure as MNIST to create a different model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6z2etLiF4Ap"
      },
      "source": [
        "# Practical Example: Audiobooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyzThy6NF4Ap"
      },
      "source": [
        "### Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2DB3dW8F4Aq"
      },
      "source": [
        "Since we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code, which is not that hard, but refers to data engineering more than machine learning.\n",
        "\n",
        "If you want to know how to do that, go through the code and the comments. In any case, this should do the trick for all datasets organized in the way: many inputs, and then 1 cell containing the targets (all supervized learning datasets).\n",
        "    \n",
        "Note that we have removed the header row, which contains the name of the categories. We simply want the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJvjTKuYF4Aq"
      },
      "source": [
        "### Extract the Data from the .csv File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpk1x9wzF4Ar"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing  # use sklearn capabilities for standardizing the inputs\n",
        "\n",
        "# load the .csv file\n",
        "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter =',')\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]  # [rows : columns] takes all columns excluding the i.d. and the targets (0 column and the last one (minus first))\n",
        "\n",
        "# record the targets \n",
        "targets_all = raw_csv_data[:,-1]  # the last column contains the targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE90zN-yF4As"
      },
      "source": [
        "### Balance the Dataset\n",
        "\n",
        "1. count the number of targets that are '1'\n",
        "2. keep as many 0s as there are 1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdqlsQ8sF4At"
      },
      "outputs": [],
      "source": [
        "# if we sum all the targets, we will get the number of targets that are 1s\n",
        "num_one_targets = int(np.sum(targets_all))\n",
        "\n",
        "# set a counter for targets which are 0s\n",
        "zero_targets_counter = 0\n",
        "\n",
        "# record the indices to be removed which for now is empty but we want it to be a list or a tuple so we put empty brackets to iterate over the data set and balance it 'i' in range targets\n",
        "indices_to_remove = []\n",
        "\n",
        "# the shape of 'targets_all' on axis = 0 , is basically the length of the vector\n",
        "# if the target at position 'i' is 0, and the number of zeroes is bigger than the number of 1s, we'll know the indices of all data points to be removed\n",
        "for i in range(targets_all.shape[0]):\n",
        "    if targets_all[i] == 0:\n",
        "       zero_targets_counter += 1\n",
        "       if zero_targets_counter > num_one_targets:\n",
        "          indices_to_remove.append(i)\n",
        "        \n",
        "# 'np.delete(array, obj to delete, axis)' -> a method that deletes an object along an axis\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwr9ZssSF4Au"
      },
      "source": [
        "### Standardize the Inputs (using Sklearn \"preprocessing\" module)\n",
        "\n",
        "* A standardized dataset will have a mean of 0 and standard deviation of 1.\n",
        "  - Standardization assumes that your data has a Gaussian (bell curve) distribution.\n",
        "* A normalized dataset will always have values that range between 0 and 1.\n",
        "  - Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh356IlLF4Av"
      },
      "outputs": [],
      "source": [
        "# 'preprocessing.scale(X)' -> a method that standardizes an array along an axis\n",
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE7J0S5dF4Av"
      },
      "source": [
        "### Shuffle the Data (using Numpy \".arange()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* a little trick is to shuffle the inputs and the targets keeps the same information but in a random order since we are batching, we must shuffle the data.\n",
        "* if ordered by date: inside a batch is homogeneous, but between batches it is very heterogeneous. This will confuse the stochastic gradient descent."
      ],
      "metadata": {
        "id": "VFSE5P9WF4Aw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjol9GpIF4Aw"
      },
      "outputs": [],
      "source": [
        "# 'np.arange([start],stop)' -> a method that returns evenly spaced values within a given interval\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "\n",
        "# 'np.random.shuffle(X)' -> a method that shuffles the numbers in a given sequence\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "# create shuffled inputs and targets to the scaled inputs and targets\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYPJ5ZFwF4Ax"
      },
      "source": [
        "### Split the Dataset into Train, Validation, and Test (80-10-10 split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4RuuPULF4Ax",
        "outputId": "ecd29651-ced1-4eae-d45f-b60caa9b916f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1810.0 3579 0.5057278569432803\n",
            "1.0 447 0.0022371364653243847\n",
            "202.0 448 0.45089285714285715\n"
          ]
        }
      ],
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "# using the 80 - 10 - 10 split\n",
        "# make sure values are integers\n",
        "train_samples_count = int(0.8*samples_count)\n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "# extract from the big data set\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[:train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[:train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "# useful to check if we have balanced the dataset\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk_DRJ_4F4Az"
      },
      "source": [
        "### Save the Three Datasets in *.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvIIQVjCF4A0"
      },
      "outputs": [],
      "source": [
        "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
        "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
        "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm1EtsPKF4A0"
      },
      "outputs": [],
      "source": [
        "# each time we reun the code in this sheet, we will preprocess the data once again (forgetting the previous preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te_AWqQCF4A0"
      },
      "outputs": [],
      "source": [
        "# This code can be used to preprocess any dataset that has two classes\n",
        "# To use this code to preprocess a dataset with more than two classifiers, must balance the data set classes instead of two"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "TensorFlow Audiobooks Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}